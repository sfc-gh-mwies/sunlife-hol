{
 "metadata": {
  "orig_nbformat": 4,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "hex_info": {
   "author": "Michelle Li",
   "project_id": "b215be44-db76-4c2d-a4cf-d34051f64ca7",
   "version": "draft",
   "exported_date": "Sun Apr 28 2024 18:35:34 GMT+0000 (Coordinated Universal Time)"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "69049a47-a548-45c1-bebd-72e9247a3624",
   "metadata": {
    "language": "python",
    "name": "banner",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import streamlit as st;\nst.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/Banner-3.png')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# Overview\n\nTasty Bytes is one of the largest food truck networks in the world with localized menu options spread across 30 major cities in 15 countries.\n**Tasty Bytes is aiming to achieve 25% YoY sales growth over 5 years.**\n\nAs Tasty Bytes Data Scientists, we have been asked to support this goal by helping our food truck drivers more intelligently pick where to park for shifts.\n**We want to direct our trucks to locations that are expected to have the highest sales on a given shift. This will maximize our daily revenue across our fleet of trucks.**\n\nTo provide this insight, we will use historical shift sales at each location to build a model. This data has been made available to us in Snowflake.\nOur model will provide the predicted sales at each location for the upcoming shift.\n\n\n\n#### **This is an introduction to Snowpark for Snowflake. We will use Snowpark to:**\n\n- Explore the data\n- Perform feature engineering\n- Train a model\n- Deploy the model in Snowflake\n\n**Why Snowpark?**\n\n- No copies or movement of data\n- Maintain governance\n- Leverage Snowflake scalable compute\n- ...and more!\n\n",
   "metadata": {
    "name": "HOL_Overview",
    "collapsed": false
   },
   "id": "216726bf-c89b-4962-b45d-6c7752a1151e"
  },
  {
   "cell_type": "code",
   "id": "6fe21437-ef72-43ca-bfa5-a8a8c612cfe0",
   "metadata": {
    "language": "python",
    "name": "Snowpark",
    "collapsed": false,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "import streamlit as st;\nst.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/snowpark_101.png')\nst.markdown(f\"Let\\'s get to know Snowpark. We will see that Snowpark makes it easy for Python users to leverage the Snowflake platform. Bringing these users into the Snowflake platform will foster collaboration and streamline architecture across all users and teams.\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Import Packages\n\nJust like the Python packages we are importing, we will import the Snowpark modules that we need.\n\n**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake.\n\n\n\n",
   "metadata": {
    "name": "Libraries",
    "collapsed": false
   },
   "id": "846843ba-a6d1-47c6-8503-c0c0d7d3d707"
  },
  {
   "cell_type": "code",
   "source": "# Import Python packages\nimport pandas as pd\nimport json\nimport sys\nimport cachetools\n#import os\n\n# Import Streamlit and viz modules\nimport plotly.express as px\nimport streamlit as st\nimport matplotlib\nimport plotly.io as pio\n\n# Import Snowflake modules\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nfrom snowflake.snowpark import Window\n\n# Import Snowflake Modeling API\nfrom snowflake.ml.modeling.pipeline import pipeline\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom snowflake.ml.registry import Registry\n\nimport warnings; warnings.simplefilter('ignore')\n\nprint(\"Imports complete.\")\n\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "metadata": {
    "name": "Import_Packages",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Imports complete.\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "08df2ef6-279b-492c-b787-9fb48e123197"
  },
  {
   "cell_type": "markdown",
   "source": "## Getting  your own environment\n\nSo that you can mess around if you feel like it and not step on each other toes, each attendee today will get a dedicated Schema and a dedicated Virtual Warehouse.\n\nSo that you can dynamically name your schema and warehouse we will use a variable as shown below.\n\nIn addition we will create a [Zero Copy Clone](https://docs.snowflake.com/user-guide/tables-storage-considerations#label-cloning-tables) of the _Analytics_ schema, so that we get an instant copy of the data without incurring any delay or storage cost. Only data that we will add or modify in the base tables will result in actual extra storage.\n\n\n\n**Value**: workload isolation and instant access to full datasets for feature engineering.\n\n\n\n\n\n",
   "metadata": {
    "name": "Personal_Environment",
    "collapsed": false
   },
   "id": "23d35a7d-77a5-435b-8aab-fd452e2e5f49"
  },
  {
   "cell_type": "code",
   "source": "#Set this up to be your name as a unique ID for the lab with some python code\nMY_ID =  '<firstname>_<lastname>'\nMY_WAREHOUSE = 'SNOWPARK_OPTIMIZED_HOL_VWH'\nMY_SCHEMA = 'SCHEMA_'+ MY_ID\n\n#create warehouse and use it\n#session.sql is a way to direct execute SQL on snowflake, we will focus on dataframes today\nsession.use_warehouse(MY_WAREHOUSE)\n\n#create schema and use it\nsession.sql(\"create or replace schema \"+MY_SCHEMA+ \" clone analytics\" ).collect()\nsession.use_schema(MY_SCHEMA)\n\nprint(session.get_current_database())\nprint(session.get_current_schema())\nprint(session.get_current_warehouse())\nprint(\"------------------------------\")\nprint(\"Session created.\")",
   "metadata": {
    "name": "HOL_Environment",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "\"FROSTBYTE_TASTY_BYTES\"\n\"SCHEMA_MICHELLE\"\n\"MICHELLE_WH\"\n------------------------------\nSession created.\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "d474460b-2216-41f0-a262-e9383ed6bf57"
  },
  {
   "cell_type": "markdown",
   "source": "## Snowpark DataFrame\n\nLet's create a Snowpark DataFrame containing our shift sales data from the **shift_sales** table in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.\n\nIf you have used PySpark in the past, this should also sound very familiar - we even have [cheat sheets](https://www.snowflake.com/en/data-cloud/snowpark/spark-to-snowpark/) to help you transition quicker!\n\n\n**Value:** Familiar representation of data for Python users.\n\n",
   "metadata": {
    "name": "Part1_Prepare_Data",
    "collapsed": false
   },
   "id": "49ba405a-8a5b-46e5-b6dc-357a4fdeb3f1"
  },
  {
   "cell_type": "code",
   "source": "snowpark_df = session.table(\"shift_sales\")",
   "metadata": {
    "name": "Snowpark_Dataframe",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "04a0be32-51d6-4abf-a1ba-51e7750d349f"
  },
  {
   "cell_type": "markdown",
   "source": "## What happened on Snowflake?\n\nLet's look at what was executed in Snowflake to create our location_df DataFrame. \n\nThe translated SQL query can be seen in the Snowsight interface under _Activity_ in the _Query History_ \n\nWe will refer to it many times over the course of this hands-on-lab so we encourage you to open it in a NEW TAB.\n\nSo, what happened behind the scenes?\n\nNothing!\n\nWhy?\n\nBecause Snowpark uses **Lazy execution**! \n\nUntil we perform an [action](https://docs.snowflake.com/en/developer-guide/snowpark/python/working-with-dataframes#performing-an-action-to-evaluate-a-dataframe) on the dataframe like a show() or collect() type of command nothing is actually pushed down to the compute.\n\n\n\n**Value:** Efficient use of compute.\n\n\n",
   "metadata": {
    "name": "Lazy_Execution",
    "collapsed": false
   },
   "id": "a9e591f9-4b1d-43b7-ae6a-25e09d4cdf29"
  },
  {
   "cell_type": "markdown",
   "source": "## Preview the Data\n\nWith our Snowpark DataFrame defined, let’s use the .show() function to take a look at the first 10 rows.\n\n\n\n**Value:** Instant access to data.\n\n",
   "metadata": {
    "name": "Data_Preview",
    "collapsed": false
   },
   "id": "7beae5de-d798-4c78-8ef9-a1c83dc721e3"
  },
  {
   "cell_type": "code",
   "source": "# Not necessary, but will allows us to look at the queries we are generating moving forward\nquery_history = session.query_history()\n\n#display some data\nsnowpark_df.show(20)",
   "metadata": {
    "name": "Data_Preview_Code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"LOCATION_ID\"  |\"CITY\"  |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|4274           |London  |2022-11-06  |NULL           |PM       |11       |0              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-07  |NULL           |PM       |11       |1              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-04  |NULL           |PM       |11       |5              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-03  |NULL           |PM       |11       |4              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-02  |NULL           |PM       |11       |3              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-08  |NULL           |PM       |11       |2              |51.496019   |-0.177579    |6                                   |8799800            |\n|4274           |London  |2022-11-05  |NULL           |PM       |11       |6              |51.496019   |-0.177579    |6                                   |8799800            |\n|14308          |Krakow  |2022-11-06  |NULL           |PM       |11       |0              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-07  |NULL           |PM       |11       |1              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-04  |NULL           |PM       |11       |5              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-03  |NULL           |PM       |11       |4              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-02  |NULL           |PM       |11       |3              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-08  |NULL           |PM       |11       |2              |50.057148   |19.944669    |83                                  |800653             |\n|14308          |Krakow  |2022-11-05  |NULL           |PM       |11       |6              |50.057148   |19.944669    |83                                  |800653             |\n|6007           |Delhi   |2022-11-06  |NULL           |PM       |11       |0              |28.536786   |77.269282    |4                                   |16349831           |\n|6007           |Delhi   |2022-11-07  |NULL           |PM       |11       |1              |28.536786   |77.269282    |4                                   |16349831           |\n|6007           |Delhi   |2022-11-04  |NULL           |PM       |11       |5              |28.536786   |77.269282    |4                                   |16349831           |\n|6007           |Delhi   |2022-11-03  |NULL           |PM       |11       |4              |28.536786   |77.269282    |4                                   |16349831           |\n|6007           |Delhi   |2022-11-02  |NULL           |PM       |11       |3              |28.536786   |77.269282    |4                                   |16349831           |\n|6007           |Delhi   |2022-11-08  |NULL           |PM       |11       |2              |28.536786   |77.269282    |4                                   |16349831           |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "c98d6a44-8e84-49f0-aadf-e68cc17ed5cd"
  },
  {
   "cell_type": "markdown",
   "source": "## Select, Filter, Sort\n\nDid you notice the Null values for \"shift_sales\"? \n\nLet's look at a single location.\n\nTo do this, we will make another Snowpark DataFrame, location_df, from the above DataFrame and we will:\n\n1. Select columns\n2. Filter to a single location ID\n3. Sort by date\n\n****\n\n**Value**: Efficient transformation pipelines using Python syntax and chained logic.\n\n",
   "metadata": {
    "name": "SnowparkDataFrame_Transformations",
    "collapsed": false
   },
   "id": "0f72f9c0-4ec2-4276-8c7c-6ce4bbcd9430"
  },
  {
   "cell_type": "code",
   "source": "# Select\nlocation_df = snowpark_df.select(\"date\", \"shift\", \"shift_sales\", \"location_id\", \"city\")\n\n# Filter (Vancouver=1135, Montreal=5319, Toronto = 1637)\nlocation_df = location_df.filter(F.col(\"location_id\") == 1135)\n#location_df = location_df.filter(F.col(\"location_id\") == 5319)\n\n# Sort\nlocation_df = location_df.order_by([\"date\", \"shift\"], ascending=[0, 0])\n\n# Display\nlocation_df.show(n=20)",
   "metadata": {
    "name": "SnowparkDataFrame_Transformations_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "------------------------------------------------------------------\n|\"DATE\"      |\"SHIFT\"  |\"SHIFT_SALES\"  |\"LOCATION_ID\"  |\"CITY\"   |\n------------------------------------------------------------------\n|2022-11-08  |PM       |NULL           |1637           |Toronto  |\n|2022-11-08  |AM       |NULL           |1637           |Toronto  |\n|2022-11-07  |PM       |NULL           |1637           |Toronto  |\n|2022-11-07  |AM       |NULL           |1637           |Toronto  |\n|2022-11-06  |PM       |NULL           |1637           |Toronto  |\n|2022-11-06  |AM       |NULL           |1637           |Toronto  |\n|2022-11-05  |PM       |NULL           |1637           |Toronto  |\n|2022-11-05  |AM       |NULL           |1637           |Toronto  |\n|2022-11-04  |PM       |NULL           |1637           |Toronto  |\n|2022-11-04  |AM       |NULL           |1637           |Toronto  |\n|2022-11-03  |PM       |NULL           |1637           |Toronto  |\n|2022-11-03  |AM       |NULL           |1637           |Toronto  |\n|2022-11-02  |PM       |NULL           |1637           |Toronto  |\n|2022-11-02  |AM       |NULL           |1637           |Toronto  |\n|2022-10-30  |PM       |23897.0        |1637           |Toronto  |\n|2022-10-23  |AM       |25627.0        |1637           |Toronto  |\n|2022-10-21  |AM       |22147.0        |1637           |Toronto  |\n|2022-10-16  |PM       |31417.0        |1637           |Toronto  |\n|2022-08-27  |PM       |24019.0        |1637           |Toronto  |\n|2022-08-24  |PM       |23256.0        |1637           |Toronto  |\n------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "5ef1eae4-9417-4bd8-ab6e-f8aa49375de7"
  },
  {
   "cell_type": "markdown",
   "source": "We can see that shift sales are populated 8 days prior to the latest date in the data. The **missing values** represent future dates that do not have shift sales yet.\n\n## Snowpark works in two main ways:\n\n1. Snowpark code translated and executed as SQL on Snowflake\n2. Python functions deployed in a secure sandbox in Snowflake\n\n\n",
   "metadata": {
    "name": "Missing_Values",
    "collapsed": false
   },
   "id": "63e7ecf2-dd14-4760-a537-4f9997f6ca33"
  },
  {
   "cell_type": "code",
   "id": "6f20ab1a-5c13-48c0-b730-2609f020ebd8",
   "metadata": {
    "language": "python",
    "name": "Snowpark_diagram",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/snowpark_overview.png')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## SQL is also an option\n\nPlease also note that if at any point there is some code that you'd rather write in SQL, you are just a click away from adding a _SQL Cell_\n\n",
   "metadata": {
    "name": "SQL_option",
    "collapsed": false
   },
   "id": "b0bc5ef9-423e-4ec4-a322-66d733a77f3e"
  },
  {
   "cell_type": "code",
   "source": "SELECT \"DATE\", \"SHIFT\", \"SHIFT_SALES\", \"LOCATION_ID\", \"CITY\" \nFROM frostbyte_tasty_bytes.analytics.shift_sales \nWHERE (\"LOCATION_ID\" = 1637) ORDER BY \"DATE\" DESC, \"SHIFT\" DESC  \nLIMIT 20;\n",
   "metadata": {
    "name": "SQL",
    "language": "sql",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "execution_count": null,
   "id": "35ccbe5a-d86d-4b1f-a53e-92bc413e9f8a"
  },
  {
   "cell_type": "code",
   "source": "session.sql(\"SELECT DATE, SHIFT, SHIFT_SALES, LOCATION_ID, CITY \\\nFROM frostbyte_tasty_bytes.analytics.shift_sales \\\nWHERE (LOCATION_ID = 1637) ORDER BY DATE DESC, SHIFT DESC \\\nLIMIT 20\").show()",
   "metadata": {
    "name": "session_SQL",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "------------------------------------------------------------------\n|\"DATE\"      |\"SHIFT\"  |\"SHIFT_SALES\"  |\"LOCATION_ID\"  |\"CITY\"   |\n------------------------------------------------------------------\n|2022-11-08  |PM       |NULL           |1637           |Toronto  |\n|2022-11-08  |AM       |NULL           |1637           |Toronto  |\n|2022-11-07  |PM       |NULL           |1637           |Toronto  |\n|2022-11-07  |AM       |NULL           |1637           |Toronto  |\n|2022-11-06  |PM       |NULL           |1637           |Toronto  |\n|2022-11-06  |AM       |NULL           |1637           |Toronto  |\n|2022-11-05  |PM       |NULL           |1637           |Toronto  |\n|2022-11-05  |AM       |NULL           |1637           |Toronto  |\n|2022-11-04  |PM       |NULL           |1637           |Toronto  |\n|2022-11-04  |AM       |NULL           |1637           |Toronto  |\n------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "917213a6-707f-42de-9f6a-05d0c463ee8a"
  },
  {
   "cell_type": "markdown",
   "source": "## Explain the Query\n\nEverything  you do in Snowflake UI can be done in code as well. Checking the latest query is no different.\n\nWhich means you don't actually need to use the Snowsight UI to see the translated SQL. You can simply query the latest record in the Query History view.\n\n**Value:** Transparent execution and compute usage.\n\n",
   "metadata": {
    "name": "Snowpark_Query_History",
    "collapsed": false
   },
   "id": "fcc96f3d-27eb-43ef-8f6d-652650922e76"
  },
  {
   "cell_type": "code",
   "source": "# Check the SQL that just executed on Snowflake!\nquery_history.queries[-1:]",
   "metadata": {
    "name": "Query_History_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "[QueryRecord(query_id='01b3c0dc-3201-3053-0002-c96200e0e3ce', sql_text='SELECT  *  FROM (SELECT DATE, SHIFT, SHIFT_SALES, LOCATION_ID, CITY FROM frostbyte_tasty_bytes.analytics.shift_sales WHERE (LOCATION_ID = 1637) ORDER BY DATE DESC, SHIFT DESC LIMIT 20) LIMIT 10')]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "de22c7d7-9212-40c3-af84-ea2dea562666"
  },
  {
   "cell_type": "markdown",
   "source": "To be fair you don't even need to run the query to understand what is going to happen! \n\nYou can use the `explain()` function to get the dataframe execution plan and ensure it looks efficient and that Snowflake will be able to prune as much data as possible.\n\n\n\n**Value:** Transparent execution and compute usage.\n\n",
   "metadata": {
    "name": "Query_Explain",
    "collapsed": false
   },
   "id": "9fcbc739-f32b-45b9-97a3-8b3d4a45bcef"
  },
  {
   "cell_type": "code",
   "source": "location_df.explain()",
   "metadata": {
    "name": "Query_Explain_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "---------DATAFRAME EXECUTION PLAN----------\nQuery List:\n1.\nSELECT \"DATE\", \"SHIFT\", \"SHIFT_SALES\", \"LOCATION_ID\", \"CITY\" FROM shift_sales WHERE (\"LOCATION_ID\" = 1637 :: INT) ORDER BY \"DATE\" DESC NULLS LAST, \"SHIFT\" DESC NULLS LAST\nLogical Execution Plan:\nGlobalStats:\n    partitionsTotal=512\n    partitionsAssigned=491\n    bytesAssigned=10372608\nOperations:\n1:0     ->Result  SHIFT_SALES.DATE, SHIFT_SALES.SHIFT, SHIFT_SALES.SHIFT_SALES, SHIFT_SALES.LOCATION_ID, SHIFT_SALES.CITY  \n1:1          ->Sort  SHIFT_SALES.DATE DESC NULLS LAST, SHIFT_SALES.SHIFT DESC NULLS LAST  \n1:2               ->Filter  SHIFT_SALES.LOCATION_ID = 1637  \n1:3                    ->TableScan  FROSTBYTE_TASTY_BYTES.SCHEMA_MICHELLE.SHIFT_SALES  LOCATION_ID, CITY, DATE, SHIFT_SALES, SHIFT  {partitionsTotal=512, partitionsAssigned=491, bytesAssigned=10372608}\n\n--------------------------------------------\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "3039381d-a8cc-40a6-99d2-37df62c72d04"
  },
  {
   "cell_type": "markdown",
   "source": "## Compare DataFrame Size\n\nLet's bring a sample of our Snowflake dataset to our local environment (AKA our browser) in a pandas DataFrame using the `to_pandas()` function. We will compare how much memory is used for the pandas DataFrame compared to the Snowpark DataFrame. As we will see, no memory is used for the Snowpark DataFrame in our Python environment. All data in the Snowpark DataFrame remains on Snowflake.\n\n\n**Value:** No copies or movement of data when working with Snowpark DataFrames.\n\n",
   "metadata": {
    "name": "CompareSFDataFramevsPandasDataFrame",
    "collapsed": false
   },
   "id": "4a05d623-7d66-464c-9095-bdc68ed6659b"
  },
  {
   "cell_type": "code",
   "source": "# Bring 10,000 rows from Snowflake to pandas\npandas_df = snowpark_df.limit(10000).to_pandas()\n\n# Get Snowpark DataFrame size\nsnowpark_size = sys.getsizeof(snowpark_df) / (1024*1024)\nprint(f\"Snowpark DataFrame Size (snowpark_df): {snowpark_size:.2f} MB\")\n\n# Get pandas DataFrame size\npandas_size = sys.getsizeof(pandas_df) / (1024*1024)\nprint(f\"Pandas DataFrame Size (pandas_df): {pandas_size:.2f} MB\")",
   "metadata": {
    "name": "CompareSFDataFramevsPandasDataFrame_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Snowpark DataFrame Size (snowpark_df): 0.00 MB\nPandas DataFrame Size (pandas_df): 0.19 MB\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "17cffefc-918b-4e14-a05a-3526feea8892"
  },
  {
   "cell_type": "code",
   "id": "cb4f0bb6-cff6-46ed-8f58-b5666c1fcc36",
   "metadata": {
    "language": "python",
    "name": "cell3",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/data_exploration.png')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "Here we will use Snowpark to explore our data. A common pattern for exploration is to use Snowpark to manipulate our data and then bring an aggregate table to our Python environment for visualization.\n\n**Value:**\n- Native Snowflake performance and scale for aggregating large datasets.\n\n\n\n",
   "metadata": {
    "name": "Data_Exploration",
    "collapsed": false
   },
   "id": "0d83d654-37fe-479a-9141-c5e4a8277221"
  },
  {
   "cell_type": "markdown",
   "source": "## How many rows are in our data?\n\nThis will give us an idea of how we might need to approach working with this data. Do we have enough data to build a meaningful model? What compute might be required? Will we need to sample the data?\n\n**What's happening where?:** Rows counted in Snowflake. No data transfer.\n\n",
   "metadata": {
    "name": "SnowparkDF_count",
    "collapsed": false
   },
   "id": "a0ec8211-bf96-4aa1-a195-9cb4743e6d16"
  },
  {
   "cell_type": "code",
   "source": "snowpark_df.count()",
   "metadata": {
    "name": "SnowparkDF_count_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "707540"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "833534d4-834b-4c07-9bb7-fe31948e6bc6"
  },
  {
   "cell_type": "markdown",
   "source": "## Let's calculate some descriptive statistics.\n\nWe use the Snowpark `describe()` function to calculate summary statistics and then bring the aggregate results into a pandas DataFrame to visualize in a formatted table.\n\n**What's happening where?:** Summary statistics calculated in Snowflake. Transfer aggregate summary statistics for client-side visualization.\n\n",
   "metadata": {
    "name": "Descriptive_Stats",
    "collapsed": false
   },
   "id": "e903e3ed-df02-437f-a5b1-a31646439a5d"
  },
  {
   "cell_type": "code",
   "source": "snowpark_df.describe().to_pandas()",
   "metadata": {
    "name": "Descriptive_Stats_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "application/vnd.hex.export+parquet": {
       "success": true,
       "exportKey": "snowpark_canada/b215be44-db76-4c2d-a4cf-d34051f64ca7/exports/760fe09f-ccb2-4c38-9706-46035fe4e07e"
      },
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SUMMARY</th>\n      <th>LOCATION_ID</th>\n      <th>CITY</th>\n      <th>SHIFT_SALES</th>\n      <th>SHIFT</th>\n      <th>MONTH</th>\n      <th>DAY_OF_WEEK</th>\n      <th>LATITUDE</th>\n      <th>LONGITUDE</th>\n      <th>COUNT_LOCATIONS_WITHIN_HALF_MILE</th>\n      <th>CITY_POPULATION</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>count</td>\n      <td>707540.000000</td>\n      <td>707540</td>\n      <td>524420.000000</td>\n      <td>707540</td>\n      <td>707540.000000</td>\n      <td>707540.000000</td>\n      <td>707540.000000</td>\n      <td>707540.000000</td>\n      <td>707540.000000</td>\n      <td>7.075400e+05</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>stddev</td>\n      <td>4199.251255</td>\n      <td>None</td>\n      <td>12077.623062</td>\n      <td>None</td>\n      <td>3.415971</td>\n      <td>2.001384</td>\n      <td>28.384873</td>\n      <td>77.663891</td>\n      <td>51.598726</td>\n      <td>4.529228e+06</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>max</td>\n      <td>15517.000000</td>\n      <td>Warsaw</td>\n      <td>78079.000000</td>\n      <td>PM</td>\n      <td>12.000000</td>\n      <td>6.000000</td>\n      <td>59.486683</td>\n      <td>151.323435</td>\n      <td>290.000000</td>\n      <td>1.634983e+07</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mean</td>\n      <td>8195.895190</td>\n      <td>None</td>\n      <td>19280.000420</td>\n      <td>None</td>\n      <td>7.649166</td>\n      <td>2.998681</td>\n      <td>31.831126</td>\n      <td>-2.486710</td>\n      <td>29.874000</td>\n      <td>4.281827e+06</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>min</td>\n      <td>1001.000000</td>\n      <td>Barcelona</td>\n      <td>3.000000</td>\n      <td>AM</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>-38.327454</td>\n      <td>-123.243134</td>\n      <td>0.000000</td>\n      <td>1.056610e+05</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "3178f289-b6fb-4144-b2ea-3de3c39473f1"
  },
  {
   "cell_type": "markdown",
   "source": "## What are the numeric columns?\n\nWe want to understand the data types in our data and how we might need to handle them in preparation for modeling. For numeric columns, this could include normalizing the data to the same scale or applying a transformation to change the distribution.\n\n**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer.\n\n",
   "metadata": {
    "name": "Numeric_Columns",
    "collapsed": false
   },
   "id": "a0694cf3-6dd8-46d6-bac9-0c5aa9c962d2"
  },
  {
   "cell_type": "code",
   "source": "# Define Snowflake numeric types\nnumeric_types = [T.DecimalType, T.DoubleType, T.FloatType, T.IntegerType, T.LongType]\n\n# Get numeric columns\nnumeric_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in numeric_types]\nnumeric_columns",
   "metadata": {
    "name": "Numeric_Columns_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "['LOCATION_ID',\n 'SHIFT_SALES',\n 'MONTH',\n 'DAY_OF_WEEK',\n 'LATITUDE',\n 'LONGITUDE',\n 'COUNT_LOCATIONS_WITHIN_HALF_MILE',\n 'CITY_POPULATION']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "372d88e6-a69c-4b78-be72-8611479fb485"
  },
  {
   "cell_type": "markdown",
   "source": "## What are the categorical columns?\n\nOur model requires all features to be numeric. We want to identify columns that we will need to transform to a numeric representation if we would like to use them as features in our model.\n\n**What's happening where?:** The Snowflake table schema is used to get metadata information about the data. No data transfer.\n\n",
   "metadata": {
    "name": "Categorical_Columns",
    "collapsed": false
   },
   "id": "2bae642f-a7ce-4d99-8d57-4a8f6b6e73b4"
  },
  {
   "cell_type": "code",
   "source": "# Define Snowflake categorical types\ncategorical_types = [T.StringType]\n\n# Get categorical columns\ncategorical_columns = [col.name for col in snowpark_df.schema.fields if type(col.datatype) in categorical_types]\ncategorical_columns",
   "metadata": {
    "name": "Categorical_Columns_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "['CITY', 'SHIFT']"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "415a134b-5d22-41f6-a1ee-f8be0a2effcd"
  },
  {
   "cell_type": "markdown",
   "source": "## What are the average shift sales (USD) by city?\n\nHere, we are trying to understand what a \"normal\" shift sale looks like. What is the span of averages across cities? Are there any outlier cities that should be removed from our training data? Is there anything unexpected in the order of cities sorted by their average shift sales?\n\n**What's happening where?:** Average sales by city calculated in Snowflake. Transfer city averages for client-side visualization.\n\n",
   "metadata": {
    "name": "Average_Shift_Sales",
    "collapsed": false
   },
   "id": "b96e7f8b-ed53-43d8-9f4c-ad638a0953c6"
  },
  {
   "cell_type": "code",
   "source": "# Group by city and average shift sales\nanalysis_df = snowpark_df.group_by(\"city\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n\n# Sort by average shift sales\nanalysis_df = analysis_df.sort(\"avg_shift_sales\", ascending=True)\n\n# Pull to pandas and plot\nanalysis_df_pandas = analysis_df.to_pandas()",
   "metadata": {
    "name": "Average_Shift_Sale_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "9da0104e-19bb-4ca2-93d8-8f56659e2af3"
  },
  {
   "cell_type": "code",
   "source": "fig = px.bar(analysis_df_pandas, x='CITY',y='AVG_SHIFT_SALES',\n      title=\"Average Shift Sales by City\")\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\nst.plotly_chart(fig, theme=\"streamlit\")",
   "metadata": {
    "name": "Average_Shift_Sales_Bar_Chart",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "bc679798-60c6-4095-bf8d-38f5014c6195"
  },
  {
   "cell_type": "code",
   "source": "st.title(\"Select a city for further analysis\")\n\nselected_city = st.selectbox('Select your city:',snowpark_df.select(\"city\").distinct())",
   "metadata": {
    "name": "City_Selection",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "1e5b77a9-e668-4c2b-a28e-3a49b95b140d"
  },
  {
   "cell_type": "code",
   "id": "562b56b2-e8ed-4019-8a31-00ea4dc48622",
   "metadata": {
    "language": "python",
    "name": "Selected_City_as_Variable",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.markdown(f\"## Looking at the {selected_city}, how many locations are there?\")\n\nst.markdown(\"Let's get to know locations and shift sales in that city. First, we will see how many location options there are in the selected city for a food truck to park.\")\n\nst.markdown(\"**What's happening where?:** Data filtered, averages calculated by location, and locations counted in Snowflake. No data transfer.\")\n\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Filter to selected_city\nanalysis_df = snowpark_df.filter(F.col(\"city\") == selected_city)\n\n# Group by location and average shift sales\nanalysis_df = analysis_df.group_by(\"location_id\").agg(F.mean(\"shift_sales\").alias(\"avg_shift_sales\"))\n\nprint(selected_city,\":\", analysis_df.count())",
   "metadata": {
    "name": "Selected_City_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "San Mateo location count: 372\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "17a520a6-8e42-47a2-9f5d-7a07e96ef57e"
  },
  {
   "cell_type": "code",
   "id": "dc64c58b-26eb-46ca-b868-92b2539b761a",
   "metadata": {
    "language": "python",
    "name": "Feature_Engineering_banner",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/feature_engineering.png')\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Feature Engineering\n\nNow let's keep revelant columns and transform columns to create features needed for our prediction model.\nTo make some of our features more useful, we will normalize them using standard preprocessing techniques, such as One-Hot Encoding. Let's fit an encoder to our data, then use it to transform the data, producing new feature columns.\n\n**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.\n\nWith SnowparkML, you can use a standard sklearn-style API to execute **fully distributed feature engineering preprocessing tasks on Snowflake compute, with zero data movement.**\n\nNotice what we haven't had to do? No tuning, maintenance, or operational overhead. We just need a role, warehouse, and access to the data.\n\n**Value**: Near-zero maintenance. Focus on the work that brings value.\n\n****\n\n## Create a Rolling Average Feature\n\nWe will use a Snowflake window function to get a **rolling shift average by location** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n\n**Step 1. Create a Window**\n\nOur window will partition the data by location and shift. It will order rows by date. It will include all rows prior to the current date of the observation it is aggregating for.\n\n",
   "metadata": {
    "name": "Feature_Engineering",
    "collapsed": false
   },
   "id": "c568877d-0d6d-48f8-8332-107f34d0f917"
  },
  {
   "cell_type": "code",
   "source": "window_by_location_all_days = (\n    Window.partition_by(\"location_id\", \"shift\")\n    .order_by(\"date\")\n    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n)",
   "metadata": {
    "name": "Create_Window",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "483ab7cb-0f96-4f67-a029-c9da2c528221"
  },
  {
   "cell_type": "markdown",
   "source": "**Step 2. Aggregate across the Window**\n\n",
   "metadata": {
    "name": "Rolling_Average",
    "collapsed": false
   },
   "id": "9cbf4ff0-26dc-4eec-b376-96f7e8143395"
  },
  {
   "cell_type": "code",
   "source": "snowpark_df = snowpark_df.with_column(\n    \"avg_location_shift_sales\", \n    F.avg(\"shift_sales\").over(window_by_location_all_days)\n)\nsnowpark_df.show(n=10)",
   "metadata": {
    "name": "Rolling_Average_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"LOCATION_ID\"  |\"CITY\"  |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|7705           |Sydney  |2020-05-27  |29446.0        |PM       |5        |3              |-33.86184   |151.20834    |111                                 |5231147            |NULL                        |\n|7705           |Sydney  |2020-06-15  |27165.0        |PM       |6        |1              |-33.86184   |151.20834    |111                                 |5231147            |29446.0                     |\n|7705           |Sydney  |2021-04-12  |42279.0        |PM       |4        |1              |-33.86184   |151.20834    |111                                 |5231147            |28305.5                     |\n|7705           |Sydney  |2021-06-24  |28219.0        |PM       |6        |4              |-33.86184   |151.20834    |111                                 |5231147            |32963.333333333336          |\n|7705           |Sydney  |2021-08-16  |27182.0        |PM       |8        |1              |-33.86184   |151.20834    |111                                 |5231147            |31777.25                    |\n|7705           |Sydney  |2021-08-29  |29430.0        |PM       |8        |0              |-33.86184   |151.20834    |111                                 |5231147            |30858.2                     |\n|7705           |Sydney  |2021-10-14  |32950.0        |PM       |10       |4              |-33.86184   |151.20834    |111                                 |5231147            |30620.166666666668          |\n|7705           |Sydney  |2021-11-30  |25361.0        |PM       |11       |2              |-33.86184   |151.20834    |111                                 |5231147            |30953.0                     |\n|7705           |Sydney  |2022-04-06  |34762.0        |PM       |4        |3              |-33.86184   |151.20834    |111                                 |5231147            |30254.0                     |\n|7705           |Sydney  |2022-05-01  |33272.0        |PM       |5        |0              |-33.86184   |151.20834    |111                                 |5231147            |30754.88888888889           |\n|7705           |Sydney  |2022-10-30  |48491.75       |PM       |10       |0              |-33.86184   |151.20834    |111                                 |5231147            |31006.6                     |\n|7705           |Sydney  |2022-11-02  |NULL           |PM       |11       |3              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-03  |NULL           |PM       |11       |4              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-04  |NULL           |PM       |11       |5              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-05  |NULL           |PM       |11       |6              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-06  |NULL           |PM       |11       |0              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-07  |NULL           |PM       |11       |1              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|7705           |Sydney  |2022-11-08  |NULL           |PM       |11       |2              |-33.86184   |151.20834    |111                                 |5231147            |32596.159090909092          |\n|6507           |Cairo   |2020-06-06  |10898.0        |PM       |6        |6              |29.996784   |30.968879    |0                                   |1010166            |NULL                        |\n|6507           |Cairo   |2020-07-08  |16140.0        |PM       |7        |3              |29.996784   |30.968879    |0                                   |1010166            |10898.0                     |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "5edbdfc4-4a16-431e-9703-eced3c58c0b4"
  },
  {
   "cell_type": "markdown",
   "source": "## Impute Missing Values with Snowpark ML\n\n\n\nThe rolling average feature we just created is missing if there are no prior shift sales at that location. We will replace those missing values with 0.\n\n",
   "metadata": {
    "name": "Impute_Missing_Values",
    "collapsed": false
   },
   "id": "cb1c3b9a-e98c-4a7e-ae07-a8cf95b7d2e1"
  },
  {
   "cell_type": "code",
   "id": "29573459-a971-4af7-8744-d1403317c61e",
   "metadata": {
    "language": "python",
    "name": "Impute_Missing_Values_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.impute import SimpleImputer\n\nnum_cols=[\"AVG_LOCATION_SHIFT_SALES\"]\nnum_cols_out=[\"AVG_LOCATION_SHIFT_SALES\"]\n\nimpute_cols = SimpleImputer(\n    input_cols=num_cols,\n    output_cols=num_cols_out,\n    strategy=\"constant\",\n    drop_input_cols=True\n    # without specifying fill_value, default is 0\n)\n\nsnowpark_df = impute_cols.fit(snowpark_df).transform(snowpark_df)\nsnowpark_df.show(10)\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d3b971a-b7aa-4cc0-b8f2-4a1447511d04",
   "metadata": {
    "language": "python",
    "name": "Encode",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.markdown(\"**One Hot Encoding with Snowpark ML**\")\nst.image('https://miro.medium.com/v2/resize:fit:1400/format:webp/0*WcPabPyXynVjGcBP')\n\nst.markdown(f\"Categorical columns need to be represented as numeric in our model. We will use the OneHotEncoder method from the PreProcessing module to encode the columh 'Shift'.\")\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# binary encoding\n#snowpark_df = snowpark_df.with_column(\"shift\", F.iff(F.col(\"shift\") == \"AM\", 1, 0))\n# snowpark_df.show(n=10)",
   "metadata": {
    "name": "Encode_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"LOCATION_ID\"  |\"CITY\"         |\"DATE\"      |\"SHIFT_SALES\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\"SHIFT\"  |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|13096          |New York City  |2020-03-09  |30055.0        |3        |1              |40.74021    |-73.987424   |30                                  |8804190            |0.0                         |0        |\n|13096          |New York City  |2020-03-18  |28108.0        |3        |3              |40.74021    |-73.987424   |30                                  |8804190            |30055.0                     |0        |\n|13096          |New York City  |2020-10-28  |28109.0        |10       |3              |40.74021    |-73.987424   |30                                  |8804190            |29081.5                     |0        |\n|13096          |New York City  |2021-05-09  |27727.0        |5        |0              |40.74021    |-73.987424   |30                                  |8804190            |28757.333333333332          |0        |\n|13096          |New York City  |2021-05-12  |29494.0        |5        |3              |40.74021    |-73.987424   |30                                  |8804190            |28499.75                    |0        |\n|13096          |New York City  |2021-08-08  |32356.0        |8        |0              |40.74021    |-73.987424   |30                                  |8804190            |28698.6                     |0        |\n|13096          |New York City  |2021-08-23  |26894.0        |8        |1              |40.74021    |-73.987424   |30                                  |8804190            |29308.166666666668          |0        |\n|13096          |New York City  |2021-08-29  |49985.0        |8        |0              |40.74021    |-73.987424   |30                                  |8804190            |28963.285714285714          |0        |\n|13096          |New York City  |2021-09-30  |32079.0        |9        |4              |40.74021    |-73.987424   |30                                  |8804190            |31591.0                     |0        |\n|13096          |New York City  |2021-10-07  |28412.0        |10       |4              |40.74021    |-73.987424   |30                                  |8804190            |31645.222222222223          |0        |\n|13096          |New York City  |2021-10-31  |15977.0        |10       |0              |40.74021    |-73.987424   |30                                  |8804190            |31321.9                     |0        |\n|13096          |New York City  |2021-11-13  |35164.0        |11       |6              |40.74021    |-73.987424   |30                                  |8804190            |29926.909090909092          |0        |\n|13096          |New York City  |2021-12-01  |25774.0        |12       |3              |40.74021    |-73.987424   |30                                  |8804190            |30363.333333333332          |0        |\n|13096          |New York City  |2021-12-22  |33118.0        |12       |3              |40.74021    |-73.987424   |30                                  |8804190            |30010.30769230769           |0        |\n|13096          |New York City  |2022-01-26  |30360.0        |1        |3              |40.74021    |-73.987424   |30                                  |8804190            |30232.285714285714          |0        |\n|13096          |New York City  |2022-03-11  |45759.0        |3        |5              |40.74021    |-73.987424   |30                                  |8804190            |30240.8                     |0        |\n|13096          |New York City  |2022-04-15  |33196.0        |4        |5              |40.74021    |-73.987424   |30                                  |8804190            |31210.6875                  |0        |\n|13096          |New York City  |2022-05-03  |33233.0        |5        |2              |40.74021    |-73.987424   |30                                  |8804190            |31327.470588235294          |0        |\n|13096          |New York City  |2022-05-04  |26726.0        |5        |3              |40.74021    |-73.987424   |30                                  |8804190            |31433.333333333332          |0        |\n|13096          |New York City  |2022-07-06  |30136.0        |7        |3              |40.74021    |-73.987424   |30                                  |8804190            |31185.57894736842           |0        |\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "ca8bee62-1dca-4f5f-b9e6-900a737b2513"
  },
  {
   "cell_type": "code",
   "id": "40d36f8f-389f-4e09-bc73-59961c96fc9f",
   "metadata": {
    "language": "python",
    "name": "OneHotEncoder_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.pipeline import pipeline\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\n\ntarget_cols = ['SHIFT']\ntarget_cols_out = ['SHIFT_OHE']\nohe = OneHotEncoder(\n    input_cols=target_cols, \n    output_cols=target_cols_out\n)\n\nohe.fit(snowpark_df)\nsnowpark_df = ohe.fit(snowpark_df).transform(snowpark_df)\nsnowpark_df.show(10)\n\n#for SiS\nsnowpark_df.write.mode(\"overwrite\").save_as_table(\"shift_sales_all_features\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Filter to Historical Data\n\nOur data includes placeholders for future data with missing shift sales. The **future data** represents the next 7 days of shifts for all locations. The **historical data** has shift sales for all locations where a food truck parked during a shift. We will only use historical data when training our model and will filter out the dates where the **shift_sales** column is missing.\n\n",
   "metadata": {
    "name": "Filter",
    "collapsed": false
   },
   "id": "4918a304-0161-4ef4-a6c1-99fc60c0fabe"
  },
  {
   "cell_type": "code",
   "source": "historical_snowpark_df = snowpark_df.filter(F.col(\"shift_sales\").is_not_null())\nhistorical_snowpark_df.show(n=10)",
   "metadata": {
    "name": "Filter_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"LOCATION_ID\"  |\"CITY\"  |\"DATE\"      |\"SHIFT_SALES\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\"SHIFT\"  |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|12697          |London  |2020-10-15  |29339.0        |10       |4              |51.536468   |-0.003512    |0                                   |8799800            |0.0                         |0        |\n|12697          |London  |2021-01-17  |50146.0        |1        |0              |51.536468   |-0.003512    |0                                   |8799800            |29339.0                     |0        |\n|12697          |London  |2021-05-18  |32287.0        |5        |2              |51.536468   |-0.003512    |0                                   |8799800            |39742.5                     |0        |\n|12697          |London  |2021-07-03  |36621.0        |7        |6              |51.536468   |-0.003512    |0                                   |8799800            |37257.333333333336          |0        |\n|12697          |London  |2021-07-20  |31329.0        |7        |2              |51.536468   |-0.003512    |0                                   |8799800            |37098.25                    |0        |\n|12697          |London  |2021-12-28  |32589.0        |12       |2              |51.536468   |-0.003512    |0                                   |8799800            |35944.4                     |0        |\n|12697          |London  |2022-01-17  |32874.0        |1        |1              |51.536468   |-0.003512    |0                                   |8799800            |35385.166666666664          |0        |\n|12697          |London  |2022-01-20  |30813.0        |1        |4              |51.536468   |-0.003512    |0                                   |8799800            |35026.42857142857           |0        |\n|12697          |London  |2022-02-17  |37527.0        |2        |4              |51.536468   |-0.003512    |0                                   |8799800            |34499.75                    |0        |\n|12697          |London  |2022-02-28  |46715.25       |2        |1              |51.536468   |-0.003512    |0                                   |8799800            |34836.11111111111           |0        |\n|12697          |London  |2022-05-06  |36805.0        |5        |5              |51.536468   |-0.003512    |0                                   |8799800            |36024.025                   |0        |\n|12697          |London  |2022-06-05  |30216.0        |6        |0              |51.536468   |-0.003512    |0                                   |8799800            |36095.02272727273           |0        |\n|12697          |London  |2022-06-08  |29149.0        |6        |3              |51.536468   |-0.003512    |0                                   |8799800            |35605.104166666664          |0        |\n|12697          |London  |2022-08-04  |35512.0        |8        |4              |51.536468   |-0.003512    |0                                   |8799800            |35108.480769230766          |0        |\n|12697          |London  |2022-08-23  |48493.25       |8        |2              |51.536468   |-0.003512    |0                                   |8799800            |35137.30357142857           |0        |\n|12697          |London  |2022-09-24  |28592.0        |9        |6              |51.536468   |-0.003512    |0                                   |8799800            |36027.7                     |0        |\n|13399          |Paris   |2021-02-14  |24450.0        |2        |0              |48.85129    |2.326518     |21                                  |2165423            |0.0                         |1        |\n|13399          |Paris   |2021-03-17  |20751.0        |3        |3              |48.85129    |2.326518     |21                                  |2165423            |24450.0                     |1        |\n|13399          |Paris   |2021-04-02  |10370.0        |4        |5              |48.85129    |2.326518     |21                                  |2165423            |22600.5                     |1        |\n|13399          |Paris   |2021-05-07  |26099.0        |5        |5              |48.85129    |2.326518     |21                                  |2165423            |18523.666666666668          |1        |\n------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "187c03bd-f944-46de-9cd7-873fd1bb57d2"
  },
  {
   "cell_type": "markdown",
   "source": "## Drop Columns\n\nDrop ID columns that will not be used in the model.\n\n",
   "metadata": {
    "name": "Drop_Columns",
    "collapsed": false
   },
   "id": "c6feff50-7ba1-4629-9420-a3638c753367"
  },
  {
   "cell_type": "code",
   "source": "historical_snowpark_df = historical_snowpark_df.drop(\"location_id\", \"city\", \"date\",\"shift\")\nhistorical_snowpark_df.show(n=10)",
   "metadata": {
    "name": "Drop_Columns_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|\"SHIFT_SALES\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"COUNT_LOCATIONS_WITHIN_HALF_MILE\"  |\"CITY_POPULATION\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\"SHIFT\"  |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n|8171.0         |8        |6              |59.328922   |18.064892    |41                                  |978770             |0.0                         |1        |\n|11863.0        |8        |2              |59.328922   |18.064892    |41                                  |978770             |8171.0                      |1        |\n|8921.25        |9        |4              |59.328922   |18.064892    |41                                  |978770             |10017.0                     |1        |\n|9393.0         |3        |4              |59.328922   |18.064892    |41                                  |978770             |9651.75                     |1        |\n|12948.0        |10       |5              |59.328922   |18.064892    |41                                  |978770             |9587.0625                   |1        |\n|12852.0        |10       |2              |59.328922   |18.064892    |41                                  |978770             |10259.25                    |1        |\n|9946.0         |11       |2              |59.328922   |18.064892    |41                                  |978770             |10691.375                   |1        |\n|3598.5         |1        |1              |59.328922   |18.064892    |41                                  |978770             |10584.892857142857          |1        |\n|8892.0         |2        |4              |59.328922   |18.064892    |41                                  |978770             |9711.59375                  |1        |\n|7687.0         |6        |1              |59.328922   |18.064892    |41                                  |978770             |9620.527777777777           |1        |\n|7809.0         |7        |3              |59.328922   |18.064892    |41                                  |978770             |9427.175                    |1        |\n|10281.0        |10       |6              |59.328922   |18.064892    |41                                  |978770             |9280.068181818182           |1        |\n|9725.0         |11       |6              |47.554506   |-122.393985  |1                                   |737015             |0.0                         |0        |\n|9113.0         |5        |3              |47.554506   |-122.393985  |1                                   |737015             |9725.0                      |0        |\n|6753.0         |5        |4              |47.554506   |-122.393985  |1                                   |737015             |9419.0                      |0        |\n|11367.0        |8        |2              |47.554506   |-122.393985  |1                                   |737015             |8530.333333333334           |0        |\n|10185.0        |8        |6              |47.554506   |-122.393985  |1                                   |737015             |9239.5                      |0        |\n|7504.0         |9        |5              |47.554506   |-122.393985  |1                                   |737015             |9428.6                      |0        |\n|4768.0         |9        |2              |47.554506   |-122.393985  |1                                   |737015             |9107.833333333334           |0        |\n|5514.0         |12       |2              |47.554506   |-122.393985  |1                                   |737015             |8487.857142857143           |0        |\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "b43f9f91-1f82-4bec-998b-97003d8c4951"
  },
  {
   "cell_type": "markdown",
   "source": "## Split Data into Training and Testing\n\nWe will use 80% of the data for model training and 20% for testing.\n\n",
   "metadata": {
    "name": "Training_Test_Datasets",
    "collapsed": false
   },
   "id": "a169935a-bdfa-414a-bf52-9b350092eaaa"
  },
  {
   "cell_type": "code",
   "source": "train_snowpark_df, test_snowpark_df = historical_snowpark_df.randomSplit([0.8, 0.2])",
   "metadata": {
    "name": "Training_Test_Datasets_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "f8ac20b6-9061-42e0-b310-1d020a87d972"
  },
  {
   "cell_type": "markdown",
   "source": "## Save Tables in Snowflake\n\nWe will save our training and test datasets to the **analytics schema** in our Snowflake account.\n\n**Value:** Eliminate redundant data processing. These tables can be re-used to train more models beyond what we are training today.\n\n",
   "metadata": {
    "name": "Persist_Data",
    "collapsed": false
   },
   "id": "f4ed0137-fe15-4a5f-812d-caf4db5151cb"
  },
  {
   "cell_type": "code",
   "source": "# Save training data\ntrain_snowpark_df.write.mode(\"overwrite\").save_as_table(\"shift_sales_train\")\n\n# Save test data\ntest_snowpark_df.write.mode(\"overwrite\").save_as_table(\"shift_sales_test\")",
   "metadata": {
    "name": "Persist_Data_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "e3bd75b6-0fee-4c90-88a4-c48bf0a0a098"
  },
  {
   "cell_type": "code",
   "id": "bef48ef4-77ce-409c-9ec8-a324f0b563af",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/model_training.png')\nst.markdown(\"**Option 1 for Model Training**\");\nst.markdown(f\"We will now use our training data to train a linear regression model on Snowflake.\")\nst.markdown(f\"We will be leveraging the deployment of Python functions into Snowflake for training and model deployment\")\nst.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/end_to_end_ml.png')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "\nHere, we see a typical data science workflow. We are finished **preparing** our data and now move on to **training in a Python stored procedure on Snowflake**. The model created from this stored procedure will be our tool for automating decisions around truck locations to maximize our revenue. We'll surface the predicted sales (model inference) on future data using a **Python user-defined function** to drive the decisions.\n\n**Snowflake Stored Procedures** work well for training because they can read data, hold an entire table in memory to find patterns, and write files (e.g. model files) back to the Snowflake database.\n\n**Snowflake User-Defined Functions** work well for inference because they return a single value for each row passed to the user-defined function. Because of this, they can easily be distributed to provide fast results.\n\n**Value**: Effortless, scalable, and secure processing **without data movement** across compute environments.\n\n",
   "metadata": {
    "name": "Model_Training",
    "collapsed": false
   },
   "id": "7352562c-9d98-42a9-8ae4-12f7226266d7"
  },
  {
   "cell_type": "code",
   "id": "31fcba97-e7c3-4f1d-b356-c0f5d8e0db77",
   "metadata": {
    "language": "python",
    "name": "End_to_End",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.markdown(\"**Simplify, accelerate and scale end-to-end AI/ML workflows**\")\nst.image(\"https://www.snowflake.com/wp-content/uploads/2023/06/Screenshot-2023-06-27-at-7.53.44-PM.png\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "session.sql(\"ALTER WAREHOUSE \" + MY_WAREHOUSE + \" SET WAREHOUSE_SIZE = 'LARGE'\").collect()",
   "metadata": {
    "name": "Scale_Up",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "[Row(status='Statement executed successfully.')]"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "15c92592-13de-433d-a3d7-528778562f33"
  },
  {
   "cell_type": "markdown",
   "source": "## Option 2:  Model Training using ML Modeling API\n\n\nLet's run our Linear Regression training job using the SnowparkML Modeling API- this will push down our model training to run on Snowflake.\n\n- The `model.fit()` function actually creates a temporary stored procedure in the background. This also means that the model training is a single-node operation but will use multiple cores within the node via the n_jobs argument if used. Be sure to use a Snowpark Optimized Warehouse if you need more memory.\n- The `model.predict()` function actually creates a temporary vectorized UDF in the background, which means the input DataFrame is batched as Pandas DataFrames and inference is parallelized across the batches of data. You can check the query history once you execute the following cell to check. The result is a dataframe with the results appended to it, not just the results like vanilla scikit-learn. \n",
   "metadata": {
    "name": "ML_Modeling_API",
    "collapsed": false
   },
   "id": "fd4523ce-53da-4941-9eb6-5f05b9da4f13"
  },
  {
   "cell_type": "code",
   "id": "26281f49-6d47-48dc-a2fb-55421a7decbf",
   "metadata": {
    "language": "python",
    "name": "ML_Modeling_API_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n#Define Linear Regression\nlinregression = LinearRegression(\n    input_cols = session.table(\"shift_sales_train\").drop(\"shift_sales\").columns,\n    label_cols = ['SHIFT_SALES'],\n    output_cols= ['PREDICTION']\n)\n\n#Train model\nlinregression.fit(session.table(\"shift_sales_train\"))\n\n#Predict\nresult = linregression.predict(session.table(\"shift_sales_train\"))\n\n# Test the model\ndf_test_pred = linregression.predict(session.table(\"shift_sales_test\"))\n#df_test_pred_pd = df_test_pred.to_pandas()\nactuals = 'SHIFT_SALES'\nprediction = 'PREDICTION'\n\nMSE = mean_squared_error(df=df_test_pred, y_true_col_names=actuals, y_pred_col_names=prediction)\nMAB = mean_absolute_error(df=df_test_pred, y_true_col_names=actuals, y_pred_col_names=prediction)\nR2 = r2_score(df=df_test_pred, y_true_col_name=actuals, y_pred_col_name=prediction)\n\nprint(f'MSE: {MSE}')\nprint(f'MAE: {MAB}')\nprint(f'R2: {R2}')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "result.select(\"SHIFT_SALES\", \"PREDICTION\").show()",
   "metadata": {
    "name": "Compare_Actuals_Predictions",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "--------------------------------------\n|\"SHIFT_SALES\"  |\"PREDICTION\"        |\n--------------------------------------\n|18218.0        |19838.588844526716  |\n|17071.0        |28855.686444856037  |\n|25681.0        |28923.19008717186   |\n|29.0           |29438.371425806647  |\n|32032.0        |30257.608191137893  |\n|33771.0        |31309.010707456735  |\n|48511.0        |31761.002816853368  |\n|16963.5        |32969.76190263197   |\n|24897.0        |31773.147323830213  |\n|39317.75       |27672.45426783916   |\n--------------------------------------\n\n"
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "id": "f8d77b14-4a4e-4ab5-b56b-8cf3230c47b6"
  },
  {
   "cell_type": "markdown",
   "source": "## Model Registry\n\nNow, with Snowpark ML's model registry, we have a Snowflake native model versioning and deployment framework. This allows us to log models, tag parameters and metrics, track metadata, create versions, and ultimately deploy models into a Snowflake warehouse or Snowpark Container Service for batch scoring tasks. \n\nSnowflake's Model Registry supports SciKitLearn, XGBoost, Pytorch, Tensorflow and MLFlow (via the pyfunc interface) models. Model Registry allows easy deployment of pre-trained open-source models from providers such as HuggingFace.\n\n***\nWhen creating a model registry, the database name is optional. If you do not specify it, MODEL_REGISTRY is the default. By using different database names, you can create multiple registries in your account for access control, lifecycle management, or other purposes.\n\nAdd a model by to the registry calling the registry’s `log_model` method. This method:\n\n- Serializes the model and uploads it to a Snowflake stage. The model, a Python object, must be serializable (“pickleable”).\n- Creates an entry in the model registry for the model, referencing the staged location.\n- Adds metadata such as description and tags to the model as specified in the `log_model` call.\n\n",
   "metadata": {
    "name": "Model_Registry",
    "collapsed": false
   },
   "id": "b2d8fea7-a12b-478a-b883-7cc3a086ace2"
  },
  {
   "cell_type": "code",
   "id": "88486e68-9978-44d0-a242-e7d3beca3e75",
   "metadata": {
    "language": "python",
    "name": "Model_Registry_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import Registry\n#suppressed warning\n\nnative_registry = Registry(session, database_name=\"FROSTBYTE_TASTY_BYTES\", schema_name=MY_SCHEMA)\n\n# Define model name\nmodel_name = \"linear_regression\"\n\n# Let's first log the very first model we trained\nmodel_v1 = native_registry.log_model(\n    model_name=model_name,\n    version_name='V1',\n    model=linregression\n)\n# Add a description\nmodel_v1.comment = \"This is the initial model of the Shift Sales Price Prediction model.\"",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_test_pred_pd = df_test_pred.to_pandas()\nactuals = df_test_pred_pd[[\"SHIFT_SALES\"]]\nprediction = df_test_pred_pd[['PREDICTION']]\n\nmodel_v1.set_metric(metric_name=\"mean_squared_error\", value=MSE)\nmodel_v1.set_metric(metric_name=\"mean_absolute_error\", value=MAB)\nmodel_v1.set_metric(metric_name=\"r2_score\", value=R2)\n",
   "metadata": {
    "name": "Add_Metrics",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "e6b78964-ea03-410d-8d13-911d32aba5ca"
  },
  {
   "cell_type": "code",
   "source": "model_df = native_registry.show_models()\nmodel_df[['created_on','database_name','schema_name','owner','name','versions']]",
   "metadata": {
    "name": "Show_Models",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "5bcfb7cb-8e41-4905-a7cb-7f92a4a52c03"
  },
  {
   "cell_type": "code",
   "id": "341b7f08-02e0-4e54-89f2-79170fc12696",
   "metadata": {
    "language": "python",
    "name": "Show_Versions",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Let's confirm model(s) that were added\nnative_registry.get_model(model_name).show_versions()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# to delete models\n#m = native_registry.get_model(\"LINEAR_REGRESSION\")\n#default_version = m.default\n#m.default = \"V2\"\n#m.delete_version(\"V1\")",
   "metadata": {
    "name": "Delete_Models",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "b4e4a90d-f47a-4edc-92ea-e69a4d101b0c"
  },
  {
   "cell_type": "markdown",
   "id": "a584a962-6045-4052-92ac-31106953afcf",
   "metadata": {
    "name": "Best_Model",
    "collapsed": false
   },
   "source": "If you have multiple versions of the model, we want the UDF to be deployed as the version with the highest R2 value"
  },
  {
   "cell_type": "code",
   "id": "9455cf7e-5ecb-405f-8427-a4ac308d818d",
   "metadata": {
    "language": "python",
    "name": "Best_Model_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "import json \nreg_df = native_registry.get_model(model_name).show_versions()\nreg_df[\"r2_score\"] = reg_df[\"metadata\"].apply(\n    lambda x: json.loads(x)[\"metrics\"][\"r2_score\"]\n)\nbest_model = reg_df.sort_values(by=\"r2_score\", ascending=False)\n\ndeployed_version = best_model[\"name\"].iloc[0]\ndeployed_version",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d5846772-46a2-423c-aabb-55741ab6c96b",
   "metadata": {
    "name": "Registry_Model_Inference",
    "collapsed": false
   },
   "source": "**Model Inference**\n\nNow we can use the best model to perform inference using the `run` method specifying the name of the function to be called and passing a Snowpark or pandas DataFrame containing the inference data. "
  },
  {
   "cell_type": "code",
   "id": "e0cc53a5-fc8a-457f-b7d9-697db44acab8",
   "metadata": {
    "language": "python",
    "name": "Registry_Model_Inference_code",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "# Set the default version to the deployed version (best model)\n# Batch Inference without needing to create a UDF\n\nm = native_registry.get_model(model_name)\nm.default = deployed_version\nmodel_version = m.default\n\nremote_prediction = model_version.run(session.table(\"shift_sales_test\"), function_name=\"predict\")\nremote_prediction.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0c731e75-16a5-4d96-a0d3-0bc50d1ff351",
   "metadata": {
    "language": "python",
    "name": "cell7",
    "collapsed": false,
    "codeCollapsed": false
   },
   "outputs": [],
   "source": "st.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/model_utilization.png');\nst.markdown(f\"Now that our model is built and deployed, let's see it in action! We will find the best place to park in Vancouver for tomorrow morning's shift.\");\nst.image('https://raw.githubusercontent.com/Snowflake-Labs/sfguide-tasty-bytes-snowpark-101-for-data-science/211360e75995b670b2b25d3204b59b42e48456cd//assets/problem_overview.png');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Predict Location Sales for the Next Shift\n\nWe will filter to the morning shift of the first future date in our data and Vancouver. We then call our scoring user-defined function to get predicted shift sales at each location.\n\n",
   "metadata": {
    "name": "Predict_Next_Shift",
    "collapsed": false
   },
   "id": "e8762a10-fe23-458b-ba86-dd9e54ee90d9"
  },
  {
   "cell_type": "code",
   "source": "# Check model predictions for holdout data SHIFT_SALES predictions for Location_IDs in Vancouver\ndate_tomorrow_df = snowpark_df.filter(\n    (F.col(\"SHIFT_SALES\").isNull())\n    & (F.col(\"SHIFT_OHE_AM\") == 1)\n    & (F.col(\"CITY\") == selected_city)\n)\n\nresults_pred = linregression.predict(date_tomorrow_df)\nresults_pred.show(20)",
   "metadata": {
    "name": "Predict_Next_Shift_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "29cef355-fcb3-41f1-85bd-68c7deb8585e"
  },
  {
   "cell_type": "markdown",
   "source": "## Democratize Data Access and Visualize Results on a Map\n\nThe yellow dots indicate higher predicted sales locations and the purple dots indicate lower predicted sales. We will use this insight to ensure that our drivers are parking at the high-value locations.\n\n**Value:** Updated predictions readily available to drive towards our corporate goals.\n\n",
   "metadata": {
    "name": "Visualize_Maps",
    "collapsed": false
   },
   "id": "6d51d95d-ddd5-4271-8927-10dab047e9be"
  },
  {
   "cell_type": "code",
   "source": "from snowflake.snowpark import Window\n\nst.title(\"Select a city to visualize top 20 locations on the map\")\n\nselected_city_map = st.text_input(\"Enter the city 👇\")\n\nsnowpark_df = session.table(\"frostbyte_tasty_bytes.analytics.shift_sales_all_features\")\n# Get the date to predict\ndate_tomorrow = snowpark_df.filter(F.col(\"shift_sales\").is_null()).select(F.min(\"date\")).collect()[0][0]\n\n# Filter to tomorrow's date and the morning shift in {{ selected_city_map }}\nlocation_predictions_df = snowpark_df.filter((F.col(\"date\") == date_tomorrow) \n                                             & (F.col(\"shift_ohe_AM\") == 1) \n                                             & (F.col(\"city\")==selected_city_map))\n# Get predictions\nlocation_predictions_df = linregression.predict(location_predictions_df).select(\n    \"city\",\n    \"location_id\", \n    \"latitude\", \n    \"longitude\",\n    \"prediction\"\n)\n\nwindow = Window.partitionBy(location_predictions_df['city']).orderBy(location_predictions_df['prediction'].desc())\nfiltered_df = location_predictions_df.select(\n    \"city\",\n    \"location_id\", \n    \"latitude\", \n    \"longitude\",\n    \"prediction\",\n    F.rank().over(window).alias('rank')).filter(F.col('rank') <= 20)\n                                               \n# Pull location predictions into a pandas DataFrame\npredictions_df = filtered_df.to_pandas()\n\nst.map(predictions_df,\n    latitude='latitude',\n    longitude='longitude',\n    size='prediction',\n)\n\n\n",
   "metadata": {
    "name": "Visualize_Maps_App",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "fc574702-6b3c-4eb5-a3cc-57cfdee7d056"
  },
  {
   "cell_type": "markdown",
   "source": "# Next Steps\n\n\n\nWe've created features, built a base model, and deployed it - all with Snowflake. What else can we do from here?\n\n\n### 1. Add new features from __Snowflake Data Marketplace:\n\n- [Snowflake Marketplace](https://www.snowflake.com/snowflake-marketplace/),\n- [Free demo listing: Weather Source, LLC: Frostbyte](https://www.snowflake.com/datasets/weather-source-llc-frostbyte/),\n- [Free demo Listing: Safegraph](https://app.snowflake.com/marketplace/listing/GZSNZL1CN82/safegraph-safegraph-frostbyte?search=frostbyte),\n\n### 2. Improve model performance using a User-Defined Table Function (UDTF) for parallel hyperparameter tuning to identify the optimal combination of hyperparameters to use in training:[¶](http://localhost:8889/notebooks/tasty_bytes_snowpark_101-release1.ipynb#2.-Improve-model-performance-using-a-User-Defined-Table-Function-(UDTF)-for-parallel-hyperparameter-tuning-to-identify-the-optimal-comibination-of-hyperparameters-to-use-in-training:)\n\n- [Blog: Parallel Hyperparameter Tuning Using Snowpark](https://medium.com/snowflake/parallel-hyperparameter-tuning-using-snowpark-53cdec2faf77),\n- [Documentation: Implementing User-Defined Table Functions (UDTFs) in Python](https://docs.snowflake.com/en/developer-guide/udf/python/udf-python-tabular-functions.html),\n\n### 3. Automate predictions with __Streams & Tasks__:\n\n- [Quickstart: Getting Started with Streams & Tasks](https://quickstarts.snowflake.com/guide/getting_started_with_streams_and_tasks/index.html?index=..%2F..index#0),\n- [Documentation: Introduction to Tasks](https://docs.snowflake.com/en/user-guide/tasks-intro.html),\n- [Documentation: Introduction to Streams](https://docs.snowflake.com/en/user-guide/streams-intro.html),\n\n### 4. Create an app interface with __Streamlit__ for truck drivers to get location predictions:\n\n- [Quickstart: Getting Started with Snowpark for Python and Streamlit](https://quickstarts.snowflake.com/guide/getting_started_with_snowpark_for_python_streamlit/index.html?index=..%2F..index#0),\n\n",
   "metadata": {
    "name": "Next_Steps",
    "collapsed": false
   },
   "id": "cbfdc6d0-1237-4500-98db-12a222895d42"
  },
  {
   "cell_type": "markdown",
   "source": "# Reset\n\nWe will run the following cell to remove the objects from Snowflake that we created in this notebook. Dropping our stage will remove the model file. We will also close our Snowflake session.\n\n",
   "metadata": {
    "name": "Reset",
    "collapsed": false
   },
   "id": "cf04f22e-34ce-41f1-bdf2-ae1410b39256"
  },
  {
   "cell_type": "code",
   "source": "#Drop training table\nsession.sql(\"DROP TABLE IF EXISTS shift_sales_train\").collect()\n\n#Drop testing table\nsession.sql(\"DROP TABLE IF EXISTS shift_sales_test\").collect()\n\nsession.sql(\"DROP SCHEMA IF EXISTS \"+ MY_SCHEMA).collect()\n\n\n#Drop training stored procedure\nsession.sql(\"DROP PROCEDURE IF EXISTS sproc_train_linreg(varchar, array, varchar, varchar)\").collect()\n\n#Drop inference user-defined function\nsession.sql(\n   \"DROP FUNCTION IF EXISTS udf_linreg_predict_location_sales(float, float, float, float, float, float, float, float)\"\n).collect()\n\n#Drop stage\nsession.sql(\"DROP STAGE IF EXISTS model_stage\").collect()\n\n#Scale down compute\nsession.sql(\"DROP WAREHOUSE \" + MY_WAREHOUSE).collect()\n\n#Close the session\nsession.close()",
   "metadata": {
    "name": "Reset_code",
    "language": "python",
    "collapsed": false,
    "codeCollapsed": false
   },
   "execution_count": null,
   "outputs": [],
   "id": "8e15ab24-11a8-4ba4-b71c-ab67e3d5049b"
  }
 ]
}